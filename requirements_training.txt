# Requirements for LoRA Fine-tuning with Unsloth
# For H200 GPU with 8-bit quantization

# Core dependencies
torch>=2.1.0
transformers>=4.36.0
datasets>=2.14.0
accelerate>=0.25.0

# LoRA and quantization
peft>=0.7.0
bitsandbytes>=0.41.0

# Note: Unsloth removed due to segfault issues
# Standard PEFT is more stable and widely supported

# Training utilities
tensorboard>=2.15.0
wandb>=0.16.0  # Optional: for experiment tracking
tqdm>=4.66.0

# Data processing
numpy>=1.24.0
pandas>=2.0.0

# HuggingFace Hub
huggingface-hub>=0.20.0

# Testing
pytest>=7.4.0
pytest-cov>=4.1.0

# Utilities
python-dotenv>=1.0.0

